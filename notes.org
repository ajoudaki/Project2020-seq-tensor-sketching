#+TITLE: Notes
* Ideas for homology search
Three parts:
** Extract short /windows/ (or rather /window achors/) from the sequences:
*Input*: A set of sequences

*Output*: A set of windows

How to select the anchors:
- Randomized
- Minimizers
- Strided at coprime intervals
- Strided at fixed non-constant intervals?


Subject to the following parameters:
- target density $d$
- window length $l$

Optimize for the following metrics
- Exact hitting rate: Number of exact window-anchor position matches
- Fuzzy hitting rate: also count matches that are /close/, for some meaning of close:
  + step function: all within distance 1 or 2 counts (because that will likely be what the data structure can handle)
    - possibly asymmetric: distance in [-1, 2] is good.
  + fuzzy: e.g. negative exponential
- Distance distribution between hits

On the following datasets:
- Synthetic with fixed uniform mutation rate
  + only substitutions
  + also indels
- Known homology dataset


** Finding close pairs
*Input*: a set of windows $W$, all of length $l$

*Output*: a set of pairs of windows $S\subset W^2$, the candidates with likely short edit
distance. Possibly sorted by decreasing likelyhood.

*** Bucketing via Bitmasking
- Repeat $r$ times:
  + Choose $P \in [w]^m$ uniform random: a subset of $k$ of the $l$ positions.
  + Map each window $w$ to $f(w) := w_P$
  + For each $x\in A$, add all pairs $(w, w')$ with $f(w) = x = f(w')$ to $S$.

Possible variations:
- Instead of choosing $m$ random positions, choose $m/k$ disjoint k-mers.
- Instead of adding $(w, w')$ to $S$ when there is at least one match, only add pairs that are in the same bucket at least $s$ out of $r$ times.

To make it work for small Edit distance (instead of Hamming distance only)
- Instead of taking $W$ as input, consider $W'$, generated by taking each $w\in W$ and for each position adding a copy $w'$ which has that one character deleted.
- this uses $l$ times more memory

To make it work for shift distance up to 1:
- Instead of adding $w$, add $w[1:]$ and $w[:-1]$, i.e. copies with the first or last character dropped.
- This uses 2x more memory.

*** Embedding
**** Trivial Hamming distance
Flattened one-hot encoding of the $w$ characters into dimension $4w$.

**** Random vectors for Hamming distance
- For each position and each character choose a random (normalized) vector in $\mathbb R^d$: $f: [w]*A \to \mathbb R^d$.
  + We could force the four vectors for each position to be orthogonal.
  + We could force correlation between $f(i, c)$ and $f(i+1, c)$, to imply only small deltas under shifting. I.e. $f'(i+1, c) = 0.9 f(i, c) + 0.1 f(i+1, c)$.
  + We could weight different characters different, so normalize $\sum_c \|f(i, c)\|^2=1$ instead of $\|f(i,c)\| = 1$.

- The hash is $h : w \mapsto \sum_i f(i, w_i)$.

- NOTE: Instead of random vectors, these could probably be trained as well.

**** Tensor Sketch Embedding
Run Tensor Sketch on the window $w$ and use the output as embedding

**** Convolutional network embedding
This is what Amir is working on. It looks quite promising and improves a lot over random initialized convolutional networks.
It would be cool to understand how exactly these trained network effect the weights, and whether we could be able to choose the right weights from the start.


*** From Embedding to Pairs
**** KD-tree
Put all points in a KD-tree, and do a query for all pairs within a given distance from each other.

- The embedding dimension should be at most 20, and typically more around 10, since each dimension will split the number of points per bucket in half.
- Higher dimension are considerably slower!

**** Random planes
As a generalization of the bimask bucketing from above, we can bucket embedding points by approximating cosine similarity:
- Repeat $r$ times:
  + Choose $k$ random vectors $v_1, \dots, v_k \in \mathbb R^d$ in $d$ dimensional space. These are $k$ planes that partition the space in $2^k$ parts.
    - This ignores norms, but we could do a product with norm bucketing as well.
    - Norm can have either lineair or logarithmic 'meaning'. This could influence how norm bucket thresholds are chosen.
  + Each embedded point $f(w)$ maps to its part: $b(f(w)) = (sign(v_1 \dot f(w)), \dots, sign(v_k \dot f(w))$.
  + Add windows with the same part $b(f(w))$ as pairs to $S$.





* Meeting notes
** June 04
- For bit-masking (for hamming distance embedding), using random-sized consecutive blocks of bits (ala kmers) may work better than random single bits
- Cyclic fourier transform may be good for shift-independent hashing
  - only need to 'normalize' for the $e^{2\pi i/n}$ rotation
- are there rotation-invariant distance metrics?
- Can we design a function (1 dim output) that has expected low deviations for low edit distance, but is practically random for large edit distance. Examples:
  - 1 dimension of TS
  - a random convolution
  - one output of a convolution + reLU nonlineairity

  What makes such a function good? Define a metric.
